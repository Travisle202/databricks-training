Slide 3
What K-Means Assumes (That's Often Wrong for Spatial Data):
"Let's start with K-means It makes critical assumptions as:
Clusters are spherical - uses Euclidean distance and tries to minimize variance within each cluster
Clusters have roughly equal density - each cluster gets 'equal attention' in the optimization
We know 'k' - we have to specify the number of clusters upfront - main limitations
The Problem? Spatial data rarely follows these rules. Think about:
Visual Demonstration:
"Look at this visualization. On the left, we have real spatial data with natural groupings. In the middle, K-means tries to force these into perfect circles. See how it:
Cuts through natural groupings to make them spherical
Forces outliers into the nearest cluster
Completely misses the density variations"
[]
The Mean Shift Alternative:
"Now, some of you might be thinking: 'What about Mean Shift?' Mean Shift is actually a step in the right direction for spatial data because:
It's density-based - clusters form around density peaks
No need to specify 'k' - finds modes automatically
Can handle arbitrary shapes - better than K-means
But Why Not Mean Shift? The Limitations:
"Despite these advantages, Mean Shift has critical drawbacks for our use cases:
Bandwidth parameter is crucial and tricky - one fixed bandwidth for all densities
Still struggles with varying densities - urban vs rural areas need different bandwidths
No explicit noise handling - everything gets clustered somewhere
The Density Bandwidth Problem:
"Imagine trying to cluster both Manhattan and Montana with the same 'neighborhood size' - it just doesn't work. Urban clusters need a small bandwidth to see individual buildings, while rural clusters need a large bandwidth to see regional patterns. Mean Shift forces us to choose one size for everything."
What Spatial Data Actually Looks Like:
"Let's talk about the data we actually work with. In our projects, we typically see:
Varying densities - downtown vs suburbs vs rural areas
Arbitrary shapes - along roads, rivers, coastlines, not circles
Significant noise - GPS errors, temporary events, outliers
Nested clusters - shopping centers within commercial districts
Unknown cluster count - we don't know how many natural groupings exist
Enter DBSCAN. 
Instead of forcing shapes or choosing one density threshold, it says: 'Find me areas where points are densely packed, and separate out the noise.' Let me show you how it works."

Slide 4

The DBSCAN Process:
"Let me walk you through how DBSCAN actually works:
Start anywhere - pick any unvisited point
Look around - find all neighbors within ε distance
Check density - if enough neighbors (≥ minPts), you've found a cluster seed
Grow the cluster - recursively add all density-connected points
Handle noise - points that never get included become noise
Slide 7
To determine density around a point, we employ circles in 2-D, spheres in 3-D, and hyper-spheres in n-dimensional spaces.

Start with red points draw orange circle around (2-D in this case) we can see that the circle intersect with these points (the orange radius is a part of the param that you can set)

The points in red are core points (close to at least 4 other pts - another param user defined)

Points in black are non core points - so this is different than noise from before - will get to that when we begin the clustering parts 

Not a Core Point: A border point does not meet the criteria to be a core
point. It has fewer than MinPts within its ε-neighbourhood.
Neighbor of a Core Point: A border point is within the ε distance of
one or more core points. In other words, it lies on the edge of a cluster,
within the radius ε of at least one core point.


Slide 8
First pick a random core point -> first cluster -> points overlap circle are added to first cluster -> extend to other core points near by 
All core point close to growing cluster are added to it then used to extend it further
All non core points that are close to growing cluster are eventually added but are not used to extend it further

So  - “One of the strengths of DBSCAN is that you don’t need to specify the number of clusters —
it discovers clusters automatically based on the data’s shape.
Another benefit is that it can find clusters of very irregular shapes — curved clusters, —
things that K-means can’t capture because of its spherical assumption.
(And finally, DBSCAN treats low-density points as noise, which makes it very robust to outliers.”)

Slide Section: Limitations
“The challenge with DBSCAN is choosing a good epsilon.
Too small, and everything becomes noise.
Too large, and everything merges into one cluster.
It also struggles when clusters have very different densities.
One cluster might be tight and dense, and another might be spread out —
a single epsilon value can’t handle both well.
And in very high-dimensional spaces, distances become less meaningful,
so DBSCAN’s performance can degrade unless we use dimensionality reduction.”

Slide 16
From this hierarchy HDBSCAN simplifies this through a process of condensation, which is guided by the min_cluster_size parameter.
It then traverses the hierarchy and whenever a cluster splits and compares their cluster sizes with min_cluster_size. If sizes of both the clusters is more than min_cluster_size, the split is considered significant else it’s considered insignificant. The smaller cluster is deemed to have “fallen out” of the larger one and is labeled as noise. The larger cluster retains the identity of the parent cluster.
This process prunes the complex hierarchy down to a much simpler tree that only shows the significant splits that result in clusters of at least min_cluster_size.

